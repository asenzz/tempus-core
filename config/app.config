# SQL stored procedures directory
SQL_PROPERTIES_DIR = ../SVRRoot/SVRPersist/postgres/

#To enable Unix socket set md5 authentication in /etc/postgresql/9.5/main/pg_hba.conf for local connections
CONNECTION_STRING = dbname=svrwave user=svrwave password=svrwave host=/var/run/postgresql
# CONNECTION_STRING = dbname=svrwave user=svrwave password=svrwave host=localhost port=5432 # Uncomment for TCP connection

# Log levels are trace, debug, info, warning, error
LOG_LEVEL = trace

# Database access type, direct postgres or async (also Postgres)
DAO_TYPE = postgres

# Prediction horizon in seconds, how far in the future to start prediction, end is the main queue resolution
PREDICTION_HORIZON = 0.2

# Number of substeps to divide every main queue time step, every step will be modelled separately
MULTISTEP_LEN = 6

# Number of feature quantisations to try while tuning feature mechanics
NUM_QUANTISATIONS = 64

# Quantisations under divisor * 2 are tested with 1 increment
QUANTISATION_DIVISOR = 30

# Online empirical mode decomposition tuning particles
OEMD_TUNE_PARTICLES = 32

# Online empirical mode decomposition tuning iterations
OEMD_TUNE_ITERATIONS = 64

# Online empirical mode decomposition column validation interleave (higher value, speeds up tuning)
OEMD_COL_ILEAVE = 2

# Online empirical mode decomposition validation skip (higher value, speeds up tuning)
TUNE_SKIP = 0

# Number of database cursors and threads when querying for arrays
DB_NUM_THREADS = 8

# Maxmimum lambda hyperparameter to search for when tuning certain kernels
TUNE_MAX_LAMBDA = 10

# Maximum tau hyperparameter to search for when tuning certain kernels
TUNE_MAX_TAU = 10

# Keep same as train interleave for best results, use perfect squares eg. 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, and 400
INTERLEAVE = 1

# Use up to N chunks for prediction
PREDICT_CHUNKS = 10000

# Autoregeresive lag multiplier, of which lag count most similar feature dimensions are used
LAG_MULTIPLIER = 100

# Shift limit for feature tuning
SHIFT_LIMIT = 500

# Number of outliers to be removed from every chunk
OUTLIER_SLACK = 0

# Last N partial of decrement distance, or dataset length, to use as reference when looking for outliers [0..1]
PREDICT_FOCUS = .05

# Alignment validation window used in OEMD tuning
ALIGN_WINDOW = 4000

# KERNEL_LENGTH 18000 is the limit for a 16GB CUDA card imposed by MAGMA
KERNEL_LENGTH = 400

# Direct IRWLS preconditioner iterations number
STABILIZE_ITERATIONS_COUNT = 64

# Hyperparameters distances tuning iterations number
TUNE_ITERATION1 = 160

# Hyperparameters distances tuning particles number
TUNE_PARTICLES1 = 160

# Hyperparameters kernel tuning iterations number
TUNE_ITERATION2 = 160

# Hyperparameters kernel tuning particles number
TUNE_PARTICLES2 = 160

# Matrix solver iterations number is number of rows multiplied by this coefficient
SOLVE_ITERATIONS_COEFFICIENT = 32

# Matrix solver number of parallel particles
SOLVE_PARTICLES = 1000

# How many weight layer per model
WEIGHT_LAYERS = 5

# Optimization bounds multiplier for weights solver, higher means faster convergence but may reduce accuracy, while lower value reuires more iterations
LIMES = .1

# The higher weight inertia is, the more tendency toword 1 chunk weights have, 0 means no intertia.
WEIGHT_INERTIA = 100

# Partial of chunks neighbouring labels to use for solving weights [0 .. 0.5)
SOLVE_RADIUS = 0.03

# Hyperparameters tune chunks at the same time
PARALLEL_CHUNKS = 4

# Chunk indexes overlap, increases number of chunks by 1 / OVERLAP
CHUNK_OVERLAP = 0

# Optimization algorithm depth when tuning model hyperparameters or weights, should be less than iterations
OPT_DEPTH = 1

# Minimum number of points per HDBScan cluster when searching for outliers
HDBS_POINTS = 10

# ANN kernel head input count is the number of features multiplied by this coefficient
NN_HEAD_COEF = 0.5

# ANN kernel hidden layer count is the number of features multiplied by this coefficient
NN_HIDE_COEF = 2

# Gradient boosting decision tree or Temporal Fusion Transformer kernel number of training epochs
K_EPOCHS = 10000

# Gradient boosting decision tree or Temporal Fusion Transformer kernel learning rate
K_LEARN_RATE = 0.001

# Max kernel dimension size the GPU can handle at once eg. m=18000 for a 16GB GPU
GPU_CHUNK = 18000
